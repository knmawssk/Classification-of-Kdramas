# -*- coding: utf-8 -*-
"""Ñ„Ð°Ð¹Ð½Ð°Ð»

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r1lihgiDaEV-Sty3g5lldbqXfUtRkaa8
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score
from itertools import product

df = pd.read_excel('/content/kdrama.xlsx')

df_clean

df_clean = df.dropna(subset=['Synopsis', 'Genre', 'Content Rating'])

df_clean['Number of Episodes'] = pd.to_numeric(df_clean['Number of Episodes'], errors='coerce')
df_clean['Rating'] = pd.to_numeric(df_clean['Rating'], errors='coerce')
df_clean['Year of release'] = pd.to_numeric(df_clean['Year of release'], errors='coerce')

for col in ['Number of Episodes', 'Rating', 'Year of release']:
    df_clean[col].fillna(df_clean[col].median(), inplace=True)

current_year = 2025
df_clean['Drama_age'] = current_year - df_clean['Year of release']

model = SentenceTransformer('all-MiniLM-L6-v2')

synopsis_embeddings = model.encode(df_clean['Synopsis'].tolist(), show_progress_bar=True)

categorical_features = ['Genre', 'Content Rating']

for col in categorical_features:
    df_clean[col] = df_clean[col].astype(str)

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

numerical_features = ['Number of Episodes', 'Rating', 'Drama_age']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numerical_features),
    ('cat', categorical_transformer, categorical_features)
])

X_meta = preprocessor.fit_transform(df_clean)

X = np.hstack([synopsis_embeddings, X_meta])

print(f"Combined feature matrix shape: {X.shape}")

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

best_k = None
best_silhouette = -1
best_calinski = -1
best_davies = float('inf')
best_kmeans_labels = None

for k in range(6, 31):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)

    sil_score = silhouette_score(X, labels)
    calinski_score = calinski_harabasz_score(X, labels)
    davies_score = davies_bouldin_score(X, labels)

    if sil_score > best_silhouette:
        best_k = k
        best_silhouette = sil_score
        best_calinski = calinski_score
        best_davies = davies_score
        best_kmeans_labels = labels

print(f"[KMeans] Best k: {best_k}, Silhouette: {best_silhouette:.4f}, Calinski-Harabasz: {best_calinski:.4f}, Davies-Bouldin: {best_davies:.4f}")

best_k_agg = None
best_silhouette_agg = -1
best_calinski_agg = -1
best_davies_agg = float('inf')
best_agg_labels = None

for k in range(6, 31):
    agg = AgglomerativeClustering(n_clusters=k, linkage='average')
    labels = agg.fit_predict(X)

    sil_score = silhouette_score(X, labels)
    calinski_score = calinski_harabasz_score(X, labels)
    davies_score = davies_bouldin_score(X, labels)

    if sil_score > best_silhouette_agg:
        best_k_agg = k
        best_silhouette_agg = sil_score
        best_calinski_agg = calinski_score
        best_davies_agg = davies_score
        best_agg_labels = labels

print(f"[Agglomerative] Best k: {best_k_agg}, Silhouette: {best_silhouette_agg:.4f}, Calinski-Harabasz: {best_calinski_agg:.4f}, Davies-Bouldin: {best_davies_agg:.4f}")

!pip install hdbscan

import hdbscan

hdb = hdbscan.HDBSCAN(min_cluster_size=5)
hdb_labels = hdb.fit_predict(X_reduced)

if len(set(hdb_labels)) > 1:
    sil_score = silhouette_score(X_reduced, hdb_labels)
    calinski_score = calinski_harabasz_score(X_reduced, hdb_labels)
    davies_score = davies_bouldin_score(X_reduced, hdb_labels)
    print(f"[HDBSCAN] Silhouette: {sil_score:.4f}, Calinski-Harabasz: {calinski_score:.4f}, Davies-Bouldin: {davies_score:.4f}")
else:
    print("[HDBSCAN] Only one cluster found, metrics not applicable.")

best_gmm_score = -1
best_gmm_k = None
best_calinski_gmm = -1
best_davies_gmm = float('inf')
best_gmm_labels = None

for k in range(6, 31):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm_labels = gmm.fit_predict(X_reduced)

    sil_score = silhouette_score(X_reduced, gmm_labels)
    calinski_score = calinski_harabasz_score(X_reduced, gmm_labels)
    davies_score = davies_bouldin_score(X_reduced, gmm_labels)

    if sil_score > best_gmm_score:
        best_gmm_score = sil_score
        best_gmm_k = k
        best_calinski_gmm = calinski_score
        best_davies_gmm = davies_score
        best_gmm_labels = gmm_labels

print(f"[GMM] Best k: {best_gmm_k}, Silhouette: {best_gmm_score:.4f}, Calinski-Harabasz: {best_calinski_gmm:.4f}, Davies-Bouldin: {best_davies_gmm:.4f}")

from sklearn.cluster import Birch

best_birch_score = -1
best_birch_k = None
best_calinski_birch = -1
best_davies_birch = float('inf')
best_birch_labels = None

for k in range(6, 31):
    birch = Birch(n_clusters=k)
    birch_labels = birch.fit_predict(X_reduced)

    sil_score = silhouette_score(X_reduced, birch_labels)
    calinski_score = calinski_harabasz_score(X_reduced, birch_labels)
    davies_score = davies_bouldin_score(X_reduced, birch_labels)

    if sil_score > best_birch_score:
        best_birch_score = sil_score
        best_birch_k = k
        best_calinski_birch = calinski_score
        best_davies_birch = davies_score
        best_birch_labels = birch_labels

print(f"[Birch] Best k: {best_birch_k}, Silhouette: {best_birch_score:.4f}, Calinski-Harabasz: {best_calinski_birch:.4f}, Davies-Bouldin: {best_davies_birch:.4f}")

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import pandas as pd
import numpy as np

results = []

models_info = [
    ('KMeans', best_kmeans_labels, best_k, X),  # Use X for KMeans evaluation
    ('Agglomerative', best_agg_labels, best_k_agg, X),  # Use X for Agglomerative evaluation
    ('GaussianMixture', best_gmm_labels, best_gmm_k, X_reduced),  # Use X_reduced
    ('Birch', best_birch_labels, best_birch_k, X_reduced),  # Use X_reduced
    ('HDBSCAN', hdb_labels, 'N/A', X_reduced)  # Use X_reduced
]

for model_name, labels, k, data_matrix in models_info:
    labels = np.array(labels)

    if model_name == 'HDBSCAN':
        # Ignore noise points labeled -1
        mask = labels != -1
        filtered_labels = labels[mask]
        filtered_data = data_matrix[mask]
        if len(set(filtered_labels)) > 1:
            sil_score = silhouette_score(filtered_data, filtered_labels)
            ch_score = calinski_harabasz_score(filtered_data, filtered_labels)
            db_score = davies_bouldin_score(filtered_data, filtered_labels)
        else:
            sil_score = None
            ch_score = None
            db_score = None
    else:
        if len(set(labels)) > 1 and -1 not in set(labels):
            sil_score = silhouette_score(data_matrix, labels)
            ch_score = calinski_harabasz_score(data_matrix, labels)
            db_score = davies_bouldin_score(data_matrix, labels)
        else:
            sil_score = None
            ch_score = None
            db_score = None

    results.append({
        'Model': model_name,
        'Best k': k,
        'Silhouette Score': sil_score,
        'Calinski-Harabasz Index': ch_score,
        'Davies-Bouldin Index': db_score
    })

results_df = pd.DataFrame(results)
print(results_df)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import silhouette_samples
import numpy as np
from sklearn.decomposition import PCA

def simple_silhouette_plot(X, labels, model_name):
    unique_labels = set(labels)
    num_clusters = len(unique_labels)
    if -1 in unique_labels:
        num_clusters -= 1

    if num_clusters > 1 and all(list(labels).count(l) >= 2 for l in unique_labels if l != -1):
        try:
            sil_vals = silhouette_samples(X, labels)
            plt.figure(figsize=(6, 4))
            plt.hist(sil_vals, bins=20, alpha=0.7)
            plt.title(f"Silhouette Score Distribution - {model_name}")
            plt.xlabel("Silhouette coefficient")
            plt.ylabel("Frequency")
            plt.show()
        except ValueError as e:
             print(f"Could not plot silhouette for {model_name} due to error: {e}")
             print("This might happen if some clusters have only one sample (after excluding noise).")
    else:
        print(f"Not enough clusters (>1) or insufficient samples per cluster to plot silhouette distribution for {model_name}.")


def simple_2d_scatter(X_2d, labels, model_name):
    if len(set(labels)) > 1:
        plt.figure(figsize=(6, 5))
        sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=[str(label) for label in labels], palette="tab10", legend='full', s=40)
        plt.title(f"Clusters by {model_name}")
        plt.xlabel("Component 1")
        plt.ylabel("Component 2")
        plt.legend(title="Cluster")
        plt.show()
    else:
         print(f"Only one cluster or all noise found for {model_name} - no scatter plot generated.")


def simple_cluster_size_bar(labels, model_name):
    unique, counts = np.unique(labels, return_counts=True)
    plt.figure(figsize=(6, 3))
    sns.barplot(x=unique, y=counts, palette="tab10")
    plt.title(f"Cluster Sizes - {model_name}")
    plt.xlabel("Cluster")
    plt.ylabel("Count")
    plt.show()

pca = PCA(n_components=2)
X_pca_2d = pca.fit_transform(X)


# Example usage for KMeans (uses X for metrics, X_pca_2d for plotting):
simple_silhouette_plot(X, best_kmeans_labels, "KMeans")
simple_2d_scatter(X_pca_2d, best_kmeans_labels, "KMeans") # Use X_pca_2d for plotting KMeans
simple_cluster_size_bar(best_kmeans_labels, "KMeans")

# Repeat similarly for Agglomerative (uses X for metrics, X_pca_2d for plotting):
simple_silhouette_plot(X, best_agg_labels, "Agglomerative")
simple_2d_scatter(X_pca_2d, best_agg_labels, "Agglomerative") # Use X_pca_2d for plotting Agglomerative
simple_cluster_size_bar(best_agg_labels, "Agglomerative")


if 'X_reduced' in globals() and X_reduced.shape[1] >= 2:
    X_reduced_2d = X_reduced[:, :2]

    # For GMM (uses X_reduced):
    simple_silhouette_plot(X_reduced, best_gmm_labels, "GMM")
    simple_2d_scatter(X_reduced_2d, best_gmm_labels, "GMM")
    simple_cluster_size_bar(best_gmm_labels, "GMM")

    # For Birch (uses X_reduced):
    simple_silhouette_plot(X_reduced, best_birch_labels, "Birch")
    simple_2d_scatter(X_reduced_2d, best_birch_labels, "Birch")
    simple_cluster_size_bar(best_birch_labels, "Birch")

    # For HDBSCAN (no fixed k, uses X_reduced)
    # HDBSCAN might produce a single cluster or label all points as noise (-1)
    simple_silhouette_plot(X_reduced, hdb_labels, "HDBSCAN")
    simple_2d_scatter(X_reduced_2d, hdb_labels, "HDBSCAN")
    simple_cluster_size_bar(hdb_labels, "HDBSCAN")
else:
    print("X_reduced not available or does not have at least 2 dimensions for plotting GMM, Birch, and HDBSCAN.")

import matplotlib.pyplot as plt
import numpy as np

# Optional: create color palette (10 distinct colors)
palette = plt.cm.get_cmap('tab10', 10)

# Plot HDBSCAN Clusters
plt.figure(figsize=(8, 6))
unique_labels = set(hdb_labels)

for label in unique_labels:
    mask = (hdb_labels == label)
    color = 'gray' if label == -1 else palette(label % 10)
    plt.scatter(X_reduced[mask, 0], X_reduced[mask, 1],
                label=f'Cluster {label}' if label != -1 else 'Noise',
                s=50, alpha=0.7, c=[color])

plt.title("HDBSCAN Clusters (PCA Projection)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.show()

# Group by cluster and show average number of episodes and rating
summary = Xing_with_clusters.groupby('HDBSCAN_Cluster')[['Number of Episodes', 'Rating']].mean()
print(summary)

Xing_with_clusters = df_clean.copy()
Xing_with_clusters['HDBSCAN_Cluster'] = hdb_labels

# Display the first few rows of each cluster (excluding noise if needed)
for cluster_id in sorted(set(hdb_labels)):
    print(f"\nðŸ“¦ Cluster {cluster_id if cluster_id != -1 else 'Noise'}:")
    display(Xing_with_clusters[Xing_with_clusters['HDBSCAN_Cluster'] == cluster_id].head(5))

# Group by cluster and show average number of episodes and rating
summary = Xing_with_clusters.groupby('HDBSCAN_Cluster')[['Number of Episodes', 'Rating']].mean()
print(summary)

